{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets hf-xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DownloadConfig\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "from huggingface_hub import hf_hub_download, HfFolder\n",
    "\n",
    "# # In terminal before running script to Avoid rate limits\n",
    "# huggingface-cli login\n",
    "# # Or in your script\n",
    "from huggingface_hub import login\n",
    "login(\"your_token_here\")  # Get token from huggingface.co/settings/tokens\n",
    "\n",
    "# Configuration\n",
    "DATASET_NAME    = \"coild/dhravani\"\n",
    "OUTPUT_DIR      = \"datasets/te/audio\"       # Optional: for local saves\n",
    "METADATA_CSV    = \"datasets/te/metadata.csv\" # Optional: for local save\n",
    "PARQUET_FILE    = \"te/te.parquet\"\n",
    "SAVE_LOCAL      = False  # Toggle to save files locally\n",
    "SAVE_ZIP        = True\n",
    "ZIP_PATH        = \"coild_dhravani_te.zip\"\n",
    "LANGUAGE        = \"te\"\n",
    "MAX_WORKERS     = 8\n",
    "MAX_RETRIES     = 3\n",
    "RETRY_BASE_DELAY = 5\n",
    "\n",
    "# Ensure local directories if saving\n",
    "if SAVE_LOCAL:\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(METADATA_CSV), exist_ok=True)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# 1. Download and load parquet metadata file\n",
    "print(\"Downloading and loading parquet metadata file...\")\n",
    "parquet_local = hf_hub_download(\n",
    "    repo_id=DATASET_NAME,\n",
    "    filename=PARQUET_FILE,\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "df = pd.read_parquet(parquet_local)\n",
    "\n",
    "# 2. Filter Telugu examples\n",
    "print(\"Filtering Telugu examples from metadata...\")\n",
    "df_filt = df[df.language == LANGUAGE].copy()\n",
    "print(f\"Found {len(df_filt)} Telugu examples\")\n",
    "\n",
    "# Prepare metadata list\n",
    "results = []\n",
    "\n",
    "# 3. Download and optionally save audio, store bytes in memory\n",
    "print(f\"Downloading {len(df_filt)} audio files into memory...\")\n",
    "\n",
    "def download_audio_bytes(src_path):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            local_cache = hf_hub_download(\n",
    "                repo_id=DATASET_NAME,\n",
    "                filename=src_path,\n",
    "                repo_type=\"dataset\"\n",
    "            )\n",
    "            with open(local_cache, 'rb') as f:\n",
    "                data = f.read()\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                delay = RETRY_BASE_DELAY * (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"Error fetching {src_path}: {e}. Retrying in {delay:.2f}s...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Failed to fetch {src_path} after {MAX_RETRIES} attempts.\")\n",
    "                return None\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {}\n",
    "    for _, row in df_filt.iterrows():\n",
    "        src = row[\"audio_path\"]\n",
    "        futures[executor.submit(download_audio_bytes, src)] = (src, row[\"transcription\"])\n",
    "\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Downloading audio bytes\"):\n",
    "        src_path, transcription = futures[future]\n",
    "        audio_bytes = future.result()\n",
    "        if audio_bytes:\n",
    "            filename = os.path.basename(src_path)\n",
    "            results.append({\"filename\": filename, \"audio_bytes\": audio_bytes, \"transcription\": transcription})\n",
    "            if SAVE_LOCAL:\n",
    "                # Save to disk if requested\n",
    "                dst = os.path.join(OUTPUT_DIR, filename)\n",
    "                with open(dst, 'wb') as wf:\n",
    "                    wf.write(audio_bytes)\n",
    "\n",
    "# 4. Build metadata DataFrame\n",
    "metadata_df = pd.DataFrame([{\"audio_path\": r[\"filename\"], \"transcription\": r[\"transcription\"]} for r in results])\n",
    "if SAVE_LOCAL:\n",
    "    metadata_df.to_csv(METADATA_CSV, index=False)\n",
    "    print(f\"✅ Saved metadata locally to {METADATA_CSV}\")\n",
    "\n",
    "# 5. Create ZIP in-memory including audio and CSV\n",
    "if SAVE_ZIP:\n",
    "    print(\"Creating in-memory ZIP archive...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        # Add audio files\n",
    "        for r in results:\n",
    "            arcname = os.path.join('audio', r['filename'])\n",
    "            zf.writestr(arcname, r['audio_bytes'])\n",
    "        # Add metadata CSV\n",
    "        csv_buffer = io.StringIO()\n",
    "        metadata_df.to_csv(csv_buffer, index=False)\n",
    "        zf.writestr('metadata.csv', csv_buffer.getvalue())\n",
    "    print(f\"✅ Created ZIP archive at {ZIP_PATH}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"✅ Finished! Processed {len(results)} files.\")\n",
    "print(f\"⏱️ Execution time: {end_time - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
